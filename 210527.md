*지도 학습(Supervised Learning)

​    ▶ 분류(Classification), 회귀(Regression) 등

*비지도 학습(Unsupervised Learning)

​    ▶ 클러스터링(Clustering), 밀도 추정(Density estimation), 차원 축소를 포함하는 시각화(Visualization) 등

*강화 학습(Reinforcement Learning)

​    ▶ 주어진 상황에서 보상을 최대로 만드는 행동을 찾는 문제

​    ▶ trade-off between exploration and exploitation



1.1 Example: Polynomial Curve Fitting

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTc5/MDAxNjIyMTAyOTg5Mjkz.vO1TpvI2_ymP-boCXngC2CYj2cfVnxTb4B6uBzo8IMYg.zXph9Fsqz5TlY2Xd7R--LI2Bt1AqhxEuum5X7JLloHog.PNG.gauss3th/image.png?type=w773)

*테일러 급수

​    ▶ 일단 알 수 없으니 가장 기본적인 형태로 예측.

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTQ5/MDAxNjIyMTAxODY4NjEx.Zh3R5zAu02FS5u2yyUis3wWNrBBsT6bFgZ7XFzXQnEMg.Gv-KEsXGHipFUEvgE3X8MNF7i5srOKQ6fS7jXxQ9aTsg.PNG.gauss3th/image.png?type=w773)

   ▶ M이 증가하면 주어진 데이터에 잘 적합은 가능하나 과적합 가능성 존재.

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTEw/MDAxNjIyMTAzMDA5OTI2.yH85S_TgbJ1zZFIsU1pmSNm9phiWeSGvmu7pswj0RaEg.uVLzqA9Gss9xsRdoVy8spQZd9WuyE1UWZrbeyL0ig9gg.PNG.gauss3th/image.png?type=w773)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTky/MDAxNjIyMTAzMDMyOTMw.r5vAtJSn6onZWd4nCKQ-fqMzYw4ATGr7-LWku44XlQ4g.RKL7Ldcbg9nkdPtRMftiFnPWvlx-el4zNcJ0-abOM5Ug.PNG.gauss3th/image.png?type=w773)

   ▶ N(데이터 수)이 증가하면 과적합을 막을 수 있음.

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjEw/MDAxNjIyMTAzMjE2NDcw.3YmG_jcDPG71SvHeZuY9z-0MkzxPvXpeNKCeI1sNI_cg.NDJEMyVHHQYPIFyYzap8V5F2M0UDjVF_qtBFJrVmoZsg.PNG.gauss3th/image.png?type=w773)



*손실 함수

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfNjAg/MDAxNjIyMTAyNTcwODkx.C3uPmigYTD0_MZk3KecSjnDf27Elq74qiWD4OCzwHbMg.0m_5FspQsnOEp0kzXnQ-xEZpLFVU5BE4bVQUVJJVULEg.PNG.gauss3th/image.png?type=w773)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjIz/MDAxNjIyMTAzMjU2MTE2.syu_0A8jiFALYrZ2yFyyhdTpvGWuayOiPtVL4RJ6YaMg.a8cC-HwTBhpH8ThhUvwXOCFlCD_3i2dO-OZ8T88QoGYg.PNG.gauss3th/image.png?type=w773)

*볼록 함수(Convex)

   ▶ 두 점을 선으로 이은 것보다 항상 아래에 있는 함수 (↔ 오목 함수)



*이차 함수(Quadratic)

   ▶ 최고 차수가 2차식인 다항식.



결론적으로, 테일러 급수가 만약 convex 할 경우 손실 함수 또한 convex가 되고 w에 대해서 quadratic이 되기 때문에 유일한 해를 가짐.



*Regulation

   ▶ 위의 과적합 예시에서 w가 값이 매우 커지거나 작아지는데 이를 막기 위한 방법 (Ridge)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfODYg/MDAxNjIyMTAzMzMxNjUx._I9DleqMdqUWbJ4pxYGqxrdDQil0jdGDa8hWl46n2Lgg.acXY2O9pCWx0dJAzoPKOeWgc0T3A9FLZ8QA9v_l9OKkg.PNG.gauss3th/image.png?type=w773)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTEz/MDAxNjIyMTAzNDMwODM4.CwIT7mPg2YBLYBk4Mqt1V1X0IiD5l4GXMaKeiH__Ywsg.sniBCVg6_xT1OftCsmw5cynag6lu8-wOdQH9xwOFkR4g.PNG.gauss3th/image.png?type=w773)

   ▶ 하지만 여전히 w에 대해 닫힌 구조(closed form)이기 때문에 위의 손실 함수에 대해서도 해를 구할 수 있음.



1.2 Probability Theory



불확실성을 어떻게 할 것인가.



*합의 법칙(Sum rule)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjg2/MDAxNjIyMTAzNjY4NTEx.mC7olorrUYJjJPKD1KzY8eoghEmoMJ6N0zWP-tnKB9Mg.fJznNRy5BL-omABdvuYbZKgcmbHshxVpjhzo2mnGpawg.PNG.gauss3th/image.png?type=w773)

*곱의 법칙(Product rule)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjEz/MDAxNjIyMTAzNzEyMjEw.e7Z_Io5v3BCitOZUtxDrimVoPxuc5in7JZPMIz1wxHIg.are8bW32w8eidIueGcCdT4e-Z7usbAGJA-CnjDm3YAQg.PNG.gauss3th/image.png?type=w773)

*베이즈 정리

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTMy/MDAxNjIyMTA0OTQyMzgy.kMbKCarcgiNWmtMw0MBbTSu6BPrqYTsBKrDauCH5pP4g.lOVEm5IApZMuujQJ_szyoIkLVt9UIRuWcMXcl6Fecjwg.PNG.gauss3th/image.png?type=w773)

1.2.1 Probability densities

*확률밀도함수(PDF)

*누적분포함수(CDF)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjg1/MDAxNjIyMTA1NDQxMzA4.hfu6azYyc1nXsg26RlSTEU2cxQFlzS9jBSk-Vnyq8Oog.YRU1KsnP7bxisyOxdGn9GxxY8Z5puFFgBGJfJV0CULgg.PNG.gauss3th/image.png?type=w773)



1.2.2 Expectations and covariances

*평균

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfOTQg/MDAxNjIyMTA1NzUwNTY0.0q1PHL4mAOMwmkInCMutIaE7T-xU6cvfBkaQLiX5KIIg.5sfHSfY5l4XM4z8vUBtc57pm28fpbTjhNMk5aWmEMc4g.PNG.gauss3th/image.png?type=w773)

*조건부 평균(기대값)

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTI0/MDAxNjIyMTA2MDMzNDE2.7kXupVEKQYmMNB_zlxnGWE647XQfC9BSpdAh1J_QNIIg.aEEizTmtPS4Ene4LKqeGplyWF87lq_YQBwp0_c4IZ8Yg.PNG.gauss3th/image.png?type=w773)

*분산

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfNzYg/MDAxNjIyMTA2MDcwNDc1.UWGxMQfO8Gnhm3zZLViSxEP6qDg44Hv4aZ5zzv3ifBAg.FM_yTtGKG_3ULX7Wtz4a4ORI_uAcB2Rug2AfQVVVk4cg.PNG.gauss3th/image.png?type=w773)

*공분산

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTI3/MDAxNjIyMTA2MDk3MzQ5.3PfGJT1SGfr6kEX7aer_gyXSRqqjY4ZN-UkFOsUeIUQg.FxN0WyFKqIt64LGmK8OTczcq-HGseKKbTUQ2OMUShMIg.PNG.gauss3th/image.png?type=w773)

1.2.3 Bayesian probabilities

*베이지안 확률 모델

   ▶ 사전 확률을 정의하고 사후 관측을 통해 사후 확률을 보정하는 방식

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjU4/MDAxNjIyMTA3MzMwNTIz.UkHRr6mAumZQ8gxeL5limAvwAQQiC7xB55HxXG3M-ucg.Voc1EFpbBoYqAPuEIk0ZJ00m2pYErQci129qGHKewNog.PNG.gauss3th/image.png?type=w773)

*위의 베이즈 정리 참고

   ▶ 사전 확률을 알고, 적절한 가정을 통해 likelihood를 정할 수 있다면 사후 확률을 알 수 있음.

1.2.4 The Gaussian distribution

*가우시안 분포

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfODIg/MDAxNjIyMTE0MDI2MzY2.8Kq4nJHrViuwWpBOlHMmMX3M332w5czuW9idSHr0ZmIg.YrKU_8qKVhPZs4HJ9RJCjLX_82xH2eKs25kBQvLUboQg.PNG.gauss3th/image.png?type=w773)

   ▶ 평균과 표준편차(분산의 제곱근)로 표현 가능.



*다변량 가우시안 분포

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfODYg/MDAxNjIyMTE0MTAyOTcy.5-aO9F74iKEKPfYdV5g0PFZLtRA6FeSUnX3zDbMMNFkg.iqGag-NOqw2O0wf0tVkREjQVgfgWFjCyYShEWAjFc-Ug.PNG.gauss3th/image.png?type=w773)

*모수 추정

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjA5/MDAxNjIyMTE1MzMyOTk5.xvgwEy8a8poeSYNSGANySU-VYmA4Asi9cZ2YVih4Aygg.51g9e5cgrf_hzDGmCU8cbSbN-a00pPNIYNAbilEQcXgg.PNG.gauss3th/image.png?type=w773)

*편향

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMTEx/MDAxNjIyMTE1Mzg3NTA4.rjyKIna125ltJHSaCgSILZgrjtHww3tFdcwBeL--auEg.QuThom1c5r5h2ZDEJjTWKVDHvBRatJWBhLursjH4aagg.PNG.gauss3th/image.png?type=w773)

   ▶ MLE로 구한 표본 평균은 모 평균과 같음. 즉, 비편향임.

   ▶ MLE로 구한 표본 분산의 평균(기대값)은 모 분산보다 과소 평가되어 있음. 즉, 편향되어 있음.

![img](https://postfiles.pstatic.net/MjAyMTA1MjdfMjg3/MDAxNjIyMTE2MDA0ODk1.sn7_x7YcjkvZ8u_-Jx6ry9S4DAEY2MrsL8QYzIMt8q4g.Mbl3-syqu-7MgYAhj74D0CP3cf2Ynl_e5YmJ0eb98xUg.PNG.gauss3th/image.png?type=w773)

   ▶ 다만 실제 모 분산 추정치는 비편향임.
