1.2.5 Curve fitting re-visited

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMzYg/MDAxNjIyNzE5MDQxOTAw.wjjfxFDZulKoYxjut1Arb1rqaM2LoRQwW5UID05Pc24g.Ap2EqHxanLvCrttAJb2k0K00oQxIipJ5DCKWKRZR6OUg.PNG.gauss3th/image.png?type=w773)

*주어진 데이터 값을 평균으로 하고 노이즈는 가우시안 분포인 것으로 해석 가능.

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjI5/MDAxNjIyNzE5OTE1NTMx.vyPCqxDGtGIzXmi1zflnAF6BX1qMwGgaz0-wOYjDslkg.59iNHnu6KUJ6PBb4HcCIm1a5IFSVz3NSXXzOjHj75tcg.PNG.gauss3th/image.png?type=w773)

*이와 같이 데이터와 모수와의 관계를 표현할 수 있으면 MLE를 통해 각각을 구할 수 있음.

1.2.6 Bayesian curve fitting

*베이지안에서는 모수를 하나의 값이 아닌 확률로 취급하기 때문에 모수를 결정하는 Hyper-parameter도 존재함

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMTYx/MDAxNjIyNzIwMDU1MDI2.feVqA18bv1YE17AVRkEVElAcXO24pE6JKZGu1cp4_bAg.9TaCsM9Xn6560Cdz2Rgk7uBq1gBy-YzB3ZI2xQraxBsg.PNG.gauss3th/image.png?type=w773)

*기존의 데이터를 통해 새로운 데이터를 예측하는 건 위의 식 하나로 정리 가능함.

1.3 Model Selection

*모델 선택을 위해 교차 검증의 방식도 활용하곤 함.

*예측력이 좋으면서 덜 복잡한 모델이 일반적으로 더 선호됨.

1.4 The Curse of Dimensionality

*고차원에서는 데이터가 Sparse 해지는 경우가 많음.

1.5 Decision Theory

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjYx/MDAxNjIyNzIxNDQ4MDgx.9nJ7sVZzNOPdtCVsdLOc88qGsQOhs1x_vG9vv56wNtgg.8VE3bBprrhbH-XLEI00fMiZIduxBOwrLklCo8x9xGUwg.PNG.gauss3th/image.png?type=w773)

1.5.1 Minimizing the misclassification rate

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjgw/MDAxNjIyNzIxOTk4NzE2.jos5vc3FOmrNJliwbvZRrg7qjGsDLF42EMe9gSAO9Sgg.Nbi0HT9Yg9gRDflTm8dQe4U2yKADFRXF6cvda3dR9QUg.PNG.gauss3th/image.png?type=w773)

*오분류할 확률을 정의할 수 있고 이를 최소화 하는 방식으로 결정해야함.

1.5.2 Minimizing the expected loss

*단순히 오분류가 아니라 각 오분류에 해당하는 비용 또는 손실을 정의.

1.5.3 The reject option

*일정 수준(Threshold)를 넘지 않을 경우 결정을 내리지 않는 기법.

1.5.4 Inference and decision

1. Generative Models

*결합 확률의 주변화 또는 사전 확률과 class-conditional density를 통해 사후 확률을 구하는 것.

*이를 통해 새로운 데이터를 생성할 수도 있게 됨.

2. Discriminative Models

*사후 확률 자체를 직접 구하는 모델

3. Discriminant Function

사후 확률을 구하는 것이 아니라 입력 공간을 직접 대입하여 결정하는 방식.



여러 기법들

1. Minimizing Risk

2. Reject Option

3. Compensating for class priors

*베이지안 이론에서 사후 확률을 다시 사전 확률로 정의하여 반복하는 등의 방식

4. Combining models

*어려운 결합 확률 구하지 말고 세부 확률을 구해서 독립이라 가정(조건부 독립)하자.

1.5.5 Loss functions for regression

*회귀에서도 비슷하게 Loss Function을 통해 문제 해결 가능

1.6 Information Theory

*확률이 적은 사건이 더 많은 정보량을 가지고 있음.

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjc4/MDAxNjIyNzIzMjQxMDMx.bDVIf8CjoBNdLguHItpKxyydjJnepB5WT-1sk9c3VBMg.f89jSfZbB_I6YwwboZlaQN0QiqsB46rJ-2FTMHmIBykg.PNG.gauss3th/image.png?type=w773)

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjM0/MDAxNjIyNzIzMjMwNTMx.I-aVBxRmx8AfMr41fQqhOLUi7aOnCD55VSwOyBuhVHog.emvGgaPdXHv4WATTfStAQnfIFpef6eECi7j12lOBs7Yg.PNG.gauss3th/image.png?type=w773)

*각각 정보의 양과 엔트로피.



*N개의 물체를 여러 개의 통에 담는 경우를 생각해보자.

*그러면 전체 경우의 수는 N!가 됨. (N개의 통에 N개의 물체를 담는 것과 같으니까)

*이때 통에 담긴 물체가 무엇인지는 중요하지 않고, 각각의 통에 몇 개가 담겼는지만 중요함.

*그러면 아래와 같은 전체 경우의 수가 나옴. 이를 multiplicity라 함.

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMTM5/MDAxNjIyNzI0MjAwNDI0.gJWlAZTAIF7iowWgxfbV7XmxiP-2zKpUgL1L6QyK9Lcg.9EEoUcVX9hknA9fI1ZYyRjaN0rIWRbkweAhSs8i67Acg.PNG.gauss3th/image.png?type=w773)

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjY5/MDAxNjIyNzIzNjE3MDQ4.JCMHt9dFWDW-QcRgUnuMk9GiBPQzF7JAcnI3vcdXTj8g.HDQ7CY2TlaJL84JoB-_JT3tNv0PZb-TbOCh5sGH4VLUg.PNG.gauss3th/image.png?type=w773)

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMTEx/MDAxNjIyNzI0NDk4MTgw.95mohvds9_7iBxcUwfOot1Yowc0XTBHQNgUeNKvzZ3gg.k0KeiZrO-a_P65BOQgSLTUS00Cpkq_i12h7MVYITAZQg.PNG.gauss3th/image.png?type=w773)

Differential Entropy

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMTYz/MDAxNjIyNzI0NzU0MjMw.6ZvG8rBo8pwyKrZO-QIry7VUJEaPLMzeP1pzMINSTy8g.oKtHsT4HGVJAntiXEorQVLFNGq1IS3VhfV7PVyKtijkg.PNG.gauss3th/image.png?type=w773)

*이산 확률일 경우 엔트로피는 확률이 균일할 때 최대값을 얻음.

*연속 확률일 경우 가우시안 분포를 따를 때 최대값을 얻음. 또한 분산이 클 수록 엔트로피도 큼.

1.6.1 Relative entropy and mutual information

![img](https://postfiles.pstatic.net/MjAyMTA2MDNfMjMg/MDAxNjIyNzI1Njc2Njky.-qbqig8LCdKCDxropa620MZwnedKz3ESbmONoy3wYZsg.aYpJPPk3MFLGH2_N98GCjoCFRlkj0DrpEYsn6bYsB4cg.PNG.gauss3th/image.png?type=w773)

*KL Divergence은 두 분포의 다름의 정도를 표현하는 식으로 p와 q 자리를 바꾸면 값도 달라짐.

*MLE의 최대화는 KL의 최소화와 같음.

*또 주변화 할 때 각각이 독립이면 그냥 곱하면 되는데 아닌 경우 그냥 곱한 것과 실제 확률간의 차이를 KL로 구할 수 있음.
